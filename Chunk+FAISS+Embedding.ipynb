{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156705,"status":"ok","timestamp":1741730133178,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"bANew18dQkV-","outputId":"db8e3cfc-0928-45b2-d392-e9fe2068bbed"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n","Collecting openai\n","  Downloading openai-1.66.2-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n","Downloading openai-1.66.2-py3-none-any.whl (567 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.3/567.3 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.61.1\n","    Uninstalling openai-1.61.1:\n","      Successfully uninstalled openai-1.61.1\n","Successfully installed openai-1.66.2\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qU langchain sentence_transformers\n","!pip install -qU langchain-huggingface langchain-community\n","!pip install -qU faiss-cpu\n","!pip install --upgrade openai\n","!pip install -qU tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_TFSGNWQxYU"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import json\n","import faiss\n","import json\n","import os\n","import pandas as pd\n","import re\n","from google.colab import drive, files\n","from typing import Any, List, Literal, Optional, Union\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import statistics\n","from scipy import stats"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26338,"status":"ok","timestamp":1741730164630,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"peYwH-MsRM6R","outputId":"6842ac70-8e63-4706-873d-71fee81d2d50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13886,"status":"ok","timestamp":1741730178517,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"H1S7hn9wRQXY","outputId":"bf0d22bc-0620-459b-cb8b-ee45c949f1f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["JSON gerado com sucesso em: /content/drive/Shareddrives/RAG Receita/normas.json\n"]}],"source":["# Caminho para a pasta dos arquivos\n","folder_path = '/content/drive/Shareddrives/RAG Receita/Scraping Normas/'\n","\n","# Dicionário para armazenar o conteúdo dos arquivos\n","data = {}\n","\n","# Itera sobre os arquivos na pasta\n","for file in os.listdir(folder_path):\n","    if file.endswith('.txt'):  # Verifica se o arquivo é um .txt\n","        file_path = os.path.join(folder_path, file)\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            content = f.read()\n","\n","        # Adiciona ao dicionário usando o nome do arquivo (sem extensão) como chave\n","        key = os.path.splitext(file)[0]\n","        data[key] = content\n","\n","# Caminho para salvar o JSON\n","output_path = '/content/drive/Shareddrives/RAG Receita/normas.json'\n","\n","# Salva o dicionário como JSON\n","with open(output_path, 'w', encoding='utf-8') as json_file:\n","    json.dump(data, json_file, ensure_ascii=False, indent=4)\n","\n","print(f\"JSON gerado com sucesso em: {output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPQyG54CgcCD"},"outputs":[],"source":["json_normas_path = '/content/drive/Shareddrives/RAG Receita/normas.json'\n","\n","with open(json_normas_path, 'r') as file:\n","    normas_json = json.load(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZvcz4H9Rbuo"},"outputs":[],"source":["delimiters = [\n","    \"CAPÍTULO\",  # Para capturar capítulos com maiúsculas (formato oficial).\n","    \"Capítulo\",  # Para capturar capítulos com maiúscula inicial (pode variar entre fontes).\n","    \"Seção\",     # Para seções (geralmente em legislações extensas).\n","    \"Subseção\",  # Para subseções (subdivisões de seções).\n","    \"Art. \",     # Para artigos principais.\n","    \"§\",         # Para parágrafos.\n","    \"Inciso\",    # Para incisos (nem sempre está explícito, mas aparece em alguns textos).\n","    \"a)\",        # Para alíneas (listas dentro de incisos ou artigos).\n","    \"b)\",        # Continuações de alíneas.\n","    \"Item\",      # Para itens (detalhamentos adicionais).\n","    \"Livro\",     # Para divisões maiores como livros em códigos.\n","    \"Título\",    # Para títulos (grandes divisões, ex: \"Título I - Das Disposições Gerais\").\n","    \"I -\", \"II -\", \"III -\", \"IV -\", \"V -\", \"VI -\", \"VII -\", \"VIII -\", \"IX -\", \"X -\", # divisão de romanos\n","    \"\\n\\n\" # Quebra de parágrafo\n","]\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    separators=delimiters,\n","    chunk_size=1000,  # Tamanho máximo do chunk em caracteres\n","    chunk_overlap=100  # Sobreposição entre chunks para contexto\n",")\n","\n","chunks_normas = []\n","\n","# Percorre cada item no JSON\n","for filename, doc in normas_json.items():\n","    # Remove a extensão \".txt\" do nome do arquivo\n","    doc_name = filename.replace(\".txt\", \"\")\n","\n","    # Divide o documento em chunks\n","    chunks = text_splitter.split_text(doc)\n","\n","    # Adiciona o nome do documento a cada chunk\n","    for chunk in chunks:\n","        # Formata cada chunk com o nome do arquivo e o conteúdo\n","        chunk_with_name = f\"{doc_name}: {chunk}\"\n","        chunks_normas.append(chunk_with_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HxGJVavokKh"},"outputs":[],"source":["def sliding_window_split(text, max_tokens=2048, stride=1024):\n","    \"\"\"\n","    Divide o texto em chunks usando uma janela deslizante (sliding window).\n","\n","    Args:\n","        text (str): O texto a ser dividido.\n","        max_tokens (int): Tamanho máximo de cada janela em tokens/caracteres.\n","        stride (int): Quantidade de avanço da janela. Menor que max_tokens para gerar sobreposição.\n","\n","    Returns:\n","        List[str]: Lista de chunks divididos pelo sliding window.\n","    \"\"\"\n","    chunks = []\n","    start = 0\n","\n","    # Continua enquanto o início da janela for menor que o tamanho do texto\n","    while start < len(text):\n","        # Final da janela\n","        end = start + max_tokens\n","        chunk = text[start:end]\n","        chunks.append(chunk)\n","\n","        # Avança com o stride\n","        start += stride\n","\n","    return chunks\n","\n","# Aplicação no seu caso\n","chunks_normas_sliding_window = []\n","\n","for filename, doc in normas_json.items():\n","    # Remove a extensão \".txt\" do nome do arquivo\n","    doc_name = filename.replace(\".txt\", \"\")\n","\n","    # Divide o texto usando a sliding window\n","    chunks = sliding_window_split(doc, max_tokens=8192, stride=4096)\n","\n","    # Adiciona o nome do documento a cada chunk\n","    for chunk in chunks:\n","        chunk_with_name = f\"{doc_name}: {chunk}\"\n","        chunks_normas_sliding_window.append(chunk_with_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L5CrM97m1o7"},"outputs":[],"source":["import tiktoken\n","model_name = \"text-embedding-3-small\"\n","tokenizer = tiktoken.encoding_for_model(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9Z69omthY6g"},"outputs":[],"source":["class HierachicalRecursiveCharacterTextSplitter(RecursiveCharacterTextSplitter):\n","\n","    def __init__(\n","        self,\n","        separators: Optional[List[str]] = None,\n","        keep_separator: Union[bool, Literal[\"start\", \"end\"]] = True,\n","        is_separator_regex: bool = False,\n","        apply_chunk_size: Optional[int] = 0,\n","        **kwargs: Any,\n","    ) -> None:\n","        \"\"\"Create a new TextSplitter.\"\"\"\n","        super().__init__(keep_separator=keep_separator, **kwargs)\n","        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n","        self._is_separator_regex = is_separator_regex\n","        self._apply_chunk_size = apply_chunk_size\n","\n","\n","\n","    def _split_text_with_regex(\n","        self, text: str, separator: str\n","    ) -> List[str]:\n","        # Now that we have the separator, split the text\n","\n","        separators = None\n","\n","        if separator:\n","            if self._keep_separator:\n","                try:\n","                    # The parentheses in the pattern keep the delimiters in the result.\n","                    _splits = re.split(f\"({separator})\", text)\n","                    splits = (\n","                        ([_splits[i] + _splits[i + 1] for i in range(0, len(_splits) - 1, 2)])\n","                        if self._keep_separator == \"end\"\n","                        else ([_splits[i] + _splits[i + 1] for i in range(1, len(_splits), 2)])\n","                    )\n","                    if len(_splits) % 2 == 0:\n","                        splits += _splits[-1:]\n","                    splits = (\n","                        (splits + [_splits[-1]])\n","                        if self._keep_separator == \"end\"\n","                        else ([_splits[0]] + splits)\n","                    )\n","\n","                    separators = [\"preamble\"] + [_splits[i].strip(\"\\n\").strip() for i in range(1, len(_splits), 2)]\n","                except IndexError as e:\n","                    print(e)\n","                    print(_splits)\n","            else:\n","                splits = re.split(separator, text)\n","        else:\n","            splits = list(text)\n","        return [s for s in splits if s != \"\"], separators\n","\n","\n","\n","    def _split_text(self, text: str, separators: List[str], steps_to_apply_chunk_size: int) -> List[str]:\n","\n","        # print(separators)\n","        # print(steps_to_apply_chunk_size)\n","        # print(self._length_function(text))\n","\n","        \"\"\"Split incoming text and return chunks.\"\"\"\n","        final_chunks = {}\n","        # Get appropriate separator to use\n","        separator = separators[-1]\n","        new_separators = []\n","        for i, _s in enumerate(separators):\n","            _separator = _s if self._is_separator_regex else re.escape(_s)\n","            if _s == \"\":\n","                separator = _s\n","                break\n","            if re.search(_separator, text):\n","                separator = _s\n","                new_separators = separators[i + 1 :]\n","                break\n","\n","        _separator = separator if self._is_separator_regex else re.escape(separator)\n","\n","        if steps_to_apply_chunk_size > 0 or self._length_function(text) > self._chunk_size:\n","            splits, separators_text = self._split_text_with_regex(text, _separator)\n","\n","            # Does not merge small chunks to respect hierarchy\n","            if not new_separators:\n","                final_chunks = dict(zip(separators_text, splits))\n","            else:\n","                for i, s in enumerate(splits):\n","                    # print(f\"new_separators={new_separators}, len(new_separators)={len(new_separators)}, len(separators)={len(separators)}\")\n","                    other_info = self._split_text(s, new_separators, steps_to_apply_chunk_size - (len(separators) - len(new_separators)))\n","                    final_chunks[separators_text[i]] = other_info\n","        else:\n","            final_chunks = text\n","\n","        return final_chunks\n","\n","\n","    def split_text(self, text: str) -> List[str]:\n","        \"\"\"Split the input text into smaller chunks based on predefined separators.\n","\n","        Args:\n","            text (str): The input text to be split.\n","\n","        Returns:\n","            List[str]: A list of text chunks obtained after splitting.\n","        \"\"\"\n","        return self._split_text(text, self._separators, self._apply_chunk_size)\n","\n","spliting_hierarchy=[\n","    \"\\nLIVRO [IVX]+\",\n","    \"\\nTÍTULO [IVX]+\",\n","    \"\\nCAPÍTULO [IVX]+|Capítulo [IVX]+|\\nCAPÍTULO ÚNICO\",\n","    \"\\nSeção [IVX]+\",\n","    \"\\nSubseção .+\",\n","    # \"\\nArt\\. \\d+[º\\. ]*|\\n.\\s+Art\\. \\d+[º\\. ]*|\\nArtigo único\\.\",\n","    \"\\nArt\\. \\d+[º\\. ]*|\\n.\\s+Art\\. \\d+[º\\. ]*\",\n","    \"\\n§ \\d+[º\\. ]*\",\n","    \"\\n[IVX]+[-\\s]*\",\n","]\n","\n","hierarchical_text_splitter = HierachicalRecursiveCharacterTextSplitter(\n","    separators=spliting_hierarchy,\n","    is_separator_regex=True,\n","    chunk_size=1000,  # Tamanho máximo do chunk em caracteres\n","    chunk_overlap=100,  # Sobreposição entre chunks para contexto\n","    apply_chunk_size=6 # Separator index to start applying the chunk size check.\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5029,"status":"ok","timestamp":1741730185131,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"in0MlFg6kG08","outputId":"15a46261-de30-45eb-abd7-171b8a077267"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing all referred legal documents:   0%|          | 1/489 [00:00<01:35,  5.12it/s]/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:1114: RuntimeWarning: divide by zero encountered in divide\n","  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n","/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py:1114: RuntimeWarning: invalid value encountered in scalar multiply\n","  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n","Processing all referred legal documents: 100%|██████████| 489/489 [00:04<00:00, 100.79it/s]\n"]}],"source":["def flatten_passages_hierarchy(passages_dictionary, current_path=\"\", passages=[]):\n","\n","    for item, value in passages_dictionary.items():\n","        item_path = current_path\n","\n","        if item == \"preamble\":\n","            if type(value) == dict:\n","                flatten_passages_hierarchy(value, current_path, passages)\n","\n","        if item_path != \"\":\n","            item_path += \"_\" + item\n","        else:\n","            item_path = item\n","\n","        if type(value) == dict:\n","            flatten_passages_hierarchy(value, item_path, passages)\n","        else:\n","            passages.append({'path': item_path,\n","                             'passage': value})\n","\n","def contar_tokens(chunk):\n","    tokens = tokenizer.encode(chunk)  # Codifica o texto e retorna a lista de tokens\n","    num_tokens = len(tokens)  # Conta o número de tokens\n","    return num_tokens\n","\n","def passages_statistics(passages_list):\n","    num_tokens_por_chunk = [contar_tokens(passage) for passage in passages_list]\n","\n","    desc_stats = stats.describe(num_tokens_por_chunk)\n","\n","    mediana_tokens = statistics.median(num_tokens_por_chunk)\n","    max_token_passage = np.argmax(num_tokens_por_chunk)\n","\n","    return({'max_tokens': desc_stats.minmax[1],\n","            'min_tokens': desc_stats.minmax[0],\n","            'mean_tokens': desc_stats.mean,\n","            'std_tokens': np.sqrt(desc_stats.variance),\n","            'skewness_tokens': desc_stats.skewness,\n","            'kurtosis_tokens': desc_stats.kurtosis,\n","            'median_tokens': statistics.median(num_tokens_por_chunk),\n","            'max_token_passage': np.argmax(num_tokens_por_chunk)})\n","\n","applied_splitter = []\n","passages_referred_docs = []\n","\n","for filename, doc in tqdm(normas_json.items(), desc=\"Processing all referred legal documents\"):\n","    # print(f\"Processing file: {filename}\")\n","\n","    # Remove a extensão \".txt\" do nome do arquivo\n","    doc_name = filename.replace(\".txt\", \"\")\n","\n","    # First try applying the hierarchical splitter\n","    chunks = hierarchical_text_splitter.split_text(doc)\n","    passages = []\n","    current_path = \"\"\n","\n","    flatten_passages_hierarchy(chunks, current_path, passages)\n","\n","    # print(chunks.keys())\n","\n","    total_statistics = passages_statistics([passage['passage'] for passage in passages])\n","\n","    # print(total_statistics)\n","\n","    #\n","    # Check if there are indications the hierarchical splitter did not work\n","    # verifying if there is any chunk longer than the maximum size.\n","    #\n","\n","    if total_statistics['max_tokens'] > 1000:\n","        # For such document, apply a regular text splitter.\n","\n","        # print(f\">>> Applying regular text splitter in file {filename}.\")\n","\n","        chunks = text_splitter.split_text(doc)\n","        applied_splitter.append({'filename': filename,\n","                                 'splitter': 'text'})\n","\n","        for passage in chunks:\n","            # Formata cada chunk com o nome do arquivo e o conteúdo\n","            passage_with_path = \"{}: {}\".format(doc_name, passage)\n","            passages_referred_docs.append(passage_with_path)\n","    else:\n","        applied_splitter.append({'filename': filename,\n","                                 'splitter': 'hierarchical'})\n","\n","        for passage in passages:\n","            # Formata cada chunk com o nome do arquivo e o conteúdo\n","            passage_with_path = \"{}: {}\".format(doc_name + \"_\" + passage['path'], passage['passage'])\n","            passages_referred_docs.append(passage_with_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1741730185137,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"xIvRSJ7Km6bS","outputId":"11c8b15f-5fa4-4ec5-9e28-d3969baa6cd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total chunked normas slidingwindow: 4351\n","Total chunked normas: 24241\n","Total chunked normas hierarchical: 24030\n"]}],"source":["print(f'Total chunked normas slidingwindow: {len(chunks_normas_sliding_window)}')\n","print(f'Total chunked normas: {len(chunks_normas)}')\n","print(f'Total chunked normas hierarchical: {len(passages_referred_docs)}')"]},{"cell_type":"code","source":["import pickle\n","\n","with open('/content/drive/Shareddrives/RAG Receita/Chunks/chunks_normas_sliding_window.pkl', \"wb\") as f:\n","    pickle.dump(chunks_normas_sliding_window, f)\n","\n","with open('/content/drive/Shareddrives/RAG Receita/Chunks/chunks_normas_recursive.pkl', \"wb\") as f:\n","    pickle.dump(chunks_normas, f)\n","\n","with open('/content/drive/Shareddrives/RAG Receita/Chunks/chunks_normas_hierarchical.pkl', \"wb\") as f:\n","    pickle.dump(passages_referred_docs, f)"],"metadata":{"id":"wp847HbbMx4E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oj7F3T2ESQE3"},"outputs":[],"source":["# Define o limite máximo de tokens permitido\n","max_tokens = 8192  # Limite do modelo\n","\n","def verificar_tamanho_chunks(chunks, max_tokens, tokenizer):\n","    \"\"\"\n","    Verifica o tamanho dos chunks em tokens e imprime aqueles que excedem o limite.\n","\n","    Args:\n","        chunks (list): Lista de chunks para verificar.\n","        max_tokens (int): Limite máximo de tokens permitido.\n","        tokenizer: Tokenizer usado para contar os tokens.\n","    \"\"\"\n","    for i, chunk in enumerate(chunks):\n","        num_tokens = len(tokenizer.encode(chunk))\n","        if num_tokens > max_tokens:\n","            print(f\"Chunk {i} excede o limite de tokens ({num_tokens} tokens):\")\n","            print(chunk[:700])  # Exibe os primeiros 300 caracteres para referência\n","            print(\"-\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5280,"status":"ok","timestamp":1740656305390,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"8FlRKyvIToo2","outputId":"338aa24b-2f6c-4df3-f93a-e5113ffb2a4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 11182 excede o limite de tokens (8633 tokens):\n","Lei nº 11.437: Seção 6 Chefe FCE 1.03 46 Assessor Técnico Especializado FCE 4.03 49 Assessor Técnico Especializado FCE 4.02 17 Assessor Técnico Especializado FCE 4.01 DIRETORIA DE GESTÃO ESTRATÉGICA 1 Diretor FCE 1.15 Coordenação-Geral 5 Coordenador-Geral FCE 1.13 Coordenação 1 Coordenador CCE 1.10 Coordenação 7 Coordenador FCE 1.10 1 Coordenador de Projeto CCE 3.10 1 Assessor Técnico Especializado FCE 4.10 Divisão 5 Chefe FCE 1.07 1 Assessor Técnico Especializado FCE 4.05 DIRETORIA DE GESTÃO DE PESSOAS 1 Diretor CCE 1.15 Coordenação-Geral 1 Coordenador-Geral CCE 1.13 Coordenação-Geral 4 Coordenador-Geral FCE 1.13 1 Gerente de Projeto FCE 3.13 Coordenação 1 Coordenador CCE 1.10 Coordenação 1\n","--------------------------------------------------\n","Chunk 12438 excede o limite de tokens (65182 tokens):\n","Instrução Normativa RFB nº 256: Art. 65. Fica formalmente revogada, sem interrupção de sua força normativa, a Instrução Normativa SRF nº 60, de 6 de junho de 2001. swap_horiz (Instrução Normativa SRF nº 60, de 06/06/01 - DISPÕE SOBRE A APURAÇÃO DO IMPOSTO SOBRE A PR - Revogação) EVERARDO MACIEL ANEXO I Tabela de Municípios, Localização e Índices de Rendimentos Mínimos para Pecuária UNIDADE DA FEDERAÇÃO: ACRE MUNICÍPIO LOCALIZAÇÃO RENDIMENTO MÍNIMO cab/ha Acrelândia Amazônia Ocidental 0,50 Assis Brasil Amazônia Ocidental 0,50 Brasiléia Amazônia Ocidental 0,50 Bujari Amazônia Ocidental 0,50 Capixaba Amazônia Ocidental 0,50 Cruzeiro do Sul Amazônia Ocidental 0,50 Epitaciolândia Amazônia Ocide\n","--------------------------------------------------\n","Chunk 23581 excede o limite de tokens (10458 tokens):\n","REsp nº 1.306.393DF; Tema Repetitivo nº 535: a) a Data de publicação a Data de julgamento PRIMEIRA TURMA SEGUNDA TURMA TERCEIRA TURMA QUARTA TURMA QUINTA TURMA SEXTA TURMA PRIMEIRA SEÇÃO SEGUNDA SEÇÃO TERCEIRA SEÇÃO CORTE ESPECIAL PRESIDÊNCIA VICE-PRESIDÊNCIA Órgão Julgador Ementa Os termos e/ou palavras digitado(s) serão pesquisados simultaneamente em um ou mais dos seguintes campos: ementa, campo de Informações Complementares e campo de Termos Auxiliares à Pesquisa. Ementa/Indexação Notas Notas são comentários feitos pela área de Jurisprudência do Tribunal. Selecione... Ações Rescisórias procedentes Acórdãos com juízo de retratação Apreensão de petrechos usualmente utilizados no tráfico de\n","--------------------------------------------------\n"]}],"source":["verificar_tamanho_chunks(chunks_normas, max_tokens, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5144,"status":"ok","timestamp":1740656310534,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"6hPkDysenOPX","outputId":"5a5a0660-c155-4534-9316-089962e3170d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 11146 excede o limite de tokens (8633 tokens):\n","Lei nº 11.437: Seção 6 Chefe FCE 1.03 46 Assessor Técnico Especializado FCE 4.03 49 Assessor Técnico Especializado FCE 4.02 17 Assessor Técnico Especializado FCE 4.01 DIRETORIA DE GESTÃO ESTRATÉGICA 1 Diretor FCE 1.15 Coordenação-Geral 5 Coordenador-Geral FCE 1.13 Coordenação 1 Coordenador CCE 1.10 Coordenação 7 Coordenador FCE 1.10 1 Coordenador de Projeto CCE 3.10 1 Assessor Técnico Especializado FCE 4.10 Divisão 5 Chefe FCE 1.07 1 Assessor Técnico Especializado FCE 4.05 DIRETORIA DE GESTÃO DE PESSOAS 1 Diretor CCE 1.15 Coordenação-Geral 1 Coordenador-Geral CCE 1.13 Coordenação-Geral 4 Coordenador-Geral FCE 1.13 1 Gerente de Projeto FCE 3.13 Coordenação 1 Coordenador CCE 1.10 Coordenação 1\n","--------------------------------------------------\n","Chunk 12362 excede o limite de tokens (65182 tokens):\n","Instrução Normativa RFB nº 256: Art. 65. Fica formalmente revogada, sem interrupção de sua força normativa, a Instrução Normativa SRF nº 60, de 6 de junho de 2001. swap_horiz (Instrução Normativa SRF nº 60, de 06/06/01 - DISPÕE SOBRE A APURAÇÃO DO IMPOSTO SOBRE A PR - Revogação) EVERARDO MACIEL ANEXO I Tabela de Municípios, Localização e Índices de Rendimentos Mínimos para Pecuária UNIDADE DA FEDERAÇÃO: ACRE MUNICÍPIO LOCALIZAÇÃO RENDIMENTO MÍNIMO cab/ha Acrelândia Amazônia Ocidental 0,50 Assis Brasil Amazônia Ocidental 0,50 Brasiléia Amazônia Ocidental 0,50 Bujari Amazônia Ocidental 0,50 Capixaba Amazônia Ocidental 0,50 Cruzeiro do Sul Amazônia Ocidental 0,50 Epitaciolândia Amazônia Ocide\n","--------------------------------------------------\n","Chunk 23406 excede o limite de tokens (10458 tokens):\n","REsp nº 1.306.393DF; Tema Repetitivo nº 535: a) a Data de publicação a Data de julgamento PRIMEIRA TURMA SEGUNDA TURMA TERCEIRA TURMA QUARTA TURMA QUINTA TURMA SEXTA TURMA PRIMEIRA SEÇÃO SEGUNDA SEÇÃO TERCEIRA SEÇÃO CORTE ESPECIAL PRESIDÊNCIA VICE-PRESIDÊNCIA Órgão Julgador Ementa Os termos e/ou palavras digitado(s) serão pesquisados simultaneamente em um ou mais dos seguintes campos: ementa, campo de Informações Complementares e campo de Termos Auxiliares à Pesquisa. Ementa/Indexação Notas Notas são comentários feitos pela área de Jurisprudência do Tribunal. Selecione... Ações Rescisórias procedentes Acórdãos com juízo de retratação Apreensão de petrechos usualmente utilizados no tráfico de\n","--------------------------------------------------\n"]}],"source":["verificar_tamanho_chunks(passages_referred_docs, max_tokens, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgwU_f4tpIQg"},"outputs":[],"source":["verificar_tamanho_chunks(chunks_normas_sliding_window, max_tokens, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7931404,"status":"ok","timestamp":1740664249417,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"X5HYY3fhhW7v","outputId":"c08041fd-bb4e-4464-b552-2acffc3833e7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Gerando embeddings: 100%|██████████| 24030/24030 [2:12:04<00:00,  3.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Índice FAISS salvo em: /content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_hierarchical.index\n"]}],"source":["import openai\n","\n","openai.api_key =\n","# Função para obter embeddings da OpenAI\n","def get_openai_embeddings(text):\n","    try:\n","        # Requisição para a API da OpenAI\n","        response = openai.embeddings.create(\n","            model=\"text-embedding-3-small\",  # Modelo de embeddings mais leve\n","            input=text\n","        )\n","\n","        # Extraindo embedding da resposta\n","        embedding = response.data[0].embedding\n","        return np.array(embedding)\n","\n","    except:\n","      return None\n","\n","# Gera embeddings para todos os chunks\n","embeddings = [\n","    embedding for chunk in tqdm(passages_referred_docs, desc=\"Gerando embeddings\")\n","    if (embedding := get_openai_embeddings(chunk)) is not None\n","]\n","\n","embeddings = np.array(embeddings).astype('float32')\n","# Cria um índice FAISS\n","d = embeddings.shape[1]  # Dimensão dos embeddings\n","index = faiss.IndexFlatL2(d)  # Usando o índice L2 (distância euclidiana)\n","index.add(embeddings)  # Adiciona os embeddings ao índice\n","# Define o caminho completo para salvar o índice no Google Drive\n","faiss_index_file = '/content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_hierarchical.index'\n","# Salva o índice no caminho especificado\n","faiss.write_index(index, faiss_index_file)\n","print(f\"Índice FAISS salvo em: {faiss_index_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8949469,"status":"ok","timestamp":1740673198887,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"},"user_tz":180},"id":"RsIjX82aZExw","outputId":"4a4305e9-eed8-42f3-c251-415c1a5e6255"},"outputs":[{"output_type":"stream","name":"stderr","text":["Gerando embeddings: 100%|██████████| 24241/24241 [2:29:05<00:00,  2.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Índice FAISS salvo em: /content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_recursive_splitter.index\n"]}],"source":["embeddings2 = [embedding for chunk in tqdm(chunks_normas, desc=\"Gerando embeddings\")\n","    if (embedding := get_openai_embeddings(chunk)) is not None\n","]\n","embeddings2 = np.array(embeddings2).astype('float32')\n","\n","d2 = embeddings2.shape[1]  # Dimensão dos embeddings\n","index2 = faiss.IndexFlatL2(d2)  # Usando o índice L2 (distância euclidiana)\n","index2.add(embeddings2)  # Adiciona os embeddings ao índice\n","faiss_index_file_normas_recursive_splitter = '/content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_recursive_splitter.index'\n","faiss.write_index(index2, faiss_index_file_normas_recursive_splitter)\n","print(f\"Índice FAISS salvo em: {faiss_index_file_normas_recursive_splitter}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLloHGKAZE5L","executionInfo":{"status":"ok","timestamp":1740674917942,"user_tz":180,"elapsed":1719054,"user":{"displayName":"Augusto Lourenço de Faria","userId":"00655066695227758399"}},"outputId":"06880df9-d57c-4919-80eb-f3766a468702"},"outputs":[{"output_type":"stream","name":"stderr","text":["Gerando embeddings: 100%|██████████| 4351/4351 [28:32<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Índice FAISS salvo em: /content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_sliding_window.index\n"]}],"source":["embeddings3 = [embedding for chunk in tqdm(chunks_normas_sliding_window, desc=\"Gerando embeddings\")\n","    if (embedding := get_openai_embeddings(chunk)) is not None\n","]\n","# Converte os embeddings para float32\n","embeddings3 = np.array(embeddings3).astype('float32')\n","d3 = embeddings3.shape[1]  # Dimensão dos embeddings\n","index3 = faiss.IndexFlatL2(d3)  # Usando o índice L2 (distância euclidiana)\n","index3.add(embeddings3)  # Adiciona os embeddings ao índice\n","faiss_index_file_normas_sliding_window = '/content/drive/Shareddrives/RAG Receita/FAISS/faiss_index_normas_sliding_window.index'\n","faiss.write_index(index3, faiss_index_file_normas_sliding_window)\n","print(f\"Índice FAISS salvo em: {faiss_index_file_normas_sliding_window}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKl31Kgrl66j"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScGDXpL9okGV"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPODHZM1MvzQ3HrHMQ7B4qp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}