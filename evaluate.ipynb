{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51d2139-0410-4874-868d-2fb2902947c9",
   "metadata": {},
   "source": [
    "Avaliação de Métricas com Rag-receita\n",
    "Este notebook demonstra como avaliar métricas utilizando o código do repositório rag-receita.\n",
    "Aqui você encontrará as etapas para importar as bibliotecas, definir funções auxiliares, executar a avaliação e, opcionalmente, consolidar os resultados e converter CSV para JSON.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86440002-6c15-472e-a419-0a1b5ac0cb6a",
   "metadata": {},
   "source": [
    "Conversão de CSV para JSON (Opcional)\n",
    "Nesta seção, convertemos um arquivo CSV em JSON, ignorando as linhas sem resposta na coluna especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022250f3-065a-4ef9-a608-adf741572ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversão concluída! JSON criado em files/data2.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_path, json_path):\n",
    "    data_list = []\n",
    "    with open(csv_path, mode='r', encoding='utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            # Ignorar se não tiver resposta na coluna especificada\n",
    "            candidate_value = row.get(\"chatgpt(4o/4mini)+search\", \"\")\n",
    "            if not candidate_value.strip():\n",
    "                continue\n",
    "            data_list.append(row)\n",
    "\n",
    "    with open(json_path, mode='w', encoding='utf-8') as f_out:\n",
    "        json.dump(data_list, f_out, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Defina os caminhos para o arquivo CSV e para o arquivo JSON de saída (ajuste conforme necessário)\n",
    "csv_path = \"files/data.csv\"\n",
    "json_path = \"files/data.json\"\n",
    "\n",
    "if not os.path.isfile(csv_path):\n",
    "    print(f\"Arquivo CSV não encontrado em {csv_path}\")\n",
    "else:\n",
    "    csv_to_json(csv_path, json_path)\n",
    "    print(f\"Conversão concluída! JSON criado em {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be32665-9191-4681-87a9-cdc7b252c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações básicas e configurações iniciais\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importações específicas do repositório\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import (\n",
    "    ResponseRelevancy,\n",
    "    FactualCorrectness,\n",
    "    SemanticSimilarity,\n",
    "    BleuScore,\n",
    "    RougeScore\n",
    ")\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Carrega variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# Definição das combinações de ROUGE\n",
    "rouge_combinations = {\n",
    "    \"rouge1\": [\"precision\", \"recall\", \"fmeasure\"],\n",
    "    \"rouge2\": [\"precision\", \"recall\", \"fmeasure\"],\n",
    "    \"rougeL\": [\"precision\", \"recall\", \"fmeasure\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b5a84-3428-43ab-88e2-01dc9e80078d",
   "metadata": {},
   "source": [
    "Funções Auxiliares\n",
    "Nesta seção, definimos as funções para inicializar as somas das métricas e para calcular a média dos resultados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba74b5e-eb46-48d1-bd49-a949195ab99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metric_sums(selected_metrics):\n",
    "    sums = {}\n",
    "    for m in selected_metrics:\n",
    "        if m == \"rouge\":\n",
    "            # Adiciona submétricas de rouge\n",
    "            for rtype, modes in rouge_combinations.items():\n",
    "                for mode in modes:\n",
    "                    key = f\"{rtype}_{mode}\"\n",
    "                    sums[key] = 0.0\n",
    "        else:\n",
    "            sums[m] = 0.0\n",
    "    return sums\n",
    "\n",
    "def average_metric_sums(metric_sums, count_items):\n",
    "    if count_items == 0:\n",
    "        return {k: 0.0 for k in metric_sums}\n",
    "    else:\n",
    "        return {k: v / count_items for k, v in metric_sums.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c0249-02b7-4384-92e6-da8b0bf98c2d",
   "metadata": {},
   "source": [
    "Função de Avaliação de Métricas\n",
    "Nesta seção, definimos a função assíncrona evaluate_metrics que processa cada item do dataset, calcula as métricas definidas e salva os resultados individualmente e de forma consolidada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764f64ce-04eb-4347-af62-38c068a7b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def evaluate_metrics(\n",
    "        data,\n",
    "        question_col=\"question_text\",\n",
    "        reference_col=\"answer\",\n",
    "        candidate_col=\"candidate\",\n",
    "        metrics=None,\n",
    "        llm_model=\"gpt-4o-mini\",\n",
    "        embedding_model=\"text-embedding-3-small\",\n",
    "        output_json=\"all_results.json\"\n",
    "):\n",
    "    results_dir = \"results_notebook\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    evaluator_llm = LangchainLLMWrapper(\n",
    "        ChatOpenAI(model=llm_model, temperature=0)\n",
    "    )\n",
    "    evaluator_embedding = OpenAIEmbeddings(model=embedding_model)\n",
    "    embeddings_wrapper = LangchainEmbeddingsWrapper(evaluator_embedding)\n",
    "\n",
    "    scorer_answer_relevancy = ResponseRelevancy(llm=evaluator_llm, embeddings=embeddings_wrapper)\n",
    "    scorer_answer_correctness = FactualCorrectness(llm=evaluator_llm)\n",
    "    scorer_semantic_similarity = SemanticSimilarity(embeddings=embeddings_wrapper)\n",
    "    bleu_scorer = BleuScore()\n",
    "\n",
    "    metric_sums = init_metric_sums(metrics)\n",
    "    count_items = 0\n",
    "    total_items = len(data)\n",
    "    item_results = []\n",
    "\n",
    "    for idx, item in enumerate(data, start=1):\n",
    "        q_number = item.get(\"question_number\", str(idx))\n",
    "        q_text = item.get(\"question_text\", item.get(question_col, \"\"))\n",
    "\n",
    "        reference = item.get(reference_col, \"\")\n",
    "        candidate = item.get(candidate_col, \"\")\n",
    "\n",
    "        if not str(candidate).strip():\n",
    "            print(f\"[{idx}/{total_items}] Questão {q_number} ignorada (candidate vazio).\")\n",
    "            continue\n",
    "\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=q_text,\n",
    "            response=str(candidate),\n",
    "            reference=str(reference)\n",
    "        )\n",
    "\n",
    "        result_item = {\n",
    "            \"question_number\": q_number,\n",
    "            \"question_text\": q_text,\n",
    "            \"reference\": reference,\n",
    "            \"candidate\": candidate\n",
    "        }\n",
    "\n",
    "        if \"answer_relevancy\" in metrics:\n",
    "            relevancy_score = await scorer_answer_relevancy.single_turn_ascore(sample)\n",
    "            result_item[\"answer_relevancy\"] = relevancy_score\n",
    "            metric_sums[\"answer_relevancy\"] += relevancy_score\n",
    "\n",
    "        if \"answer_correctness\" in metrics:\n",
    "            correctness_score = await scorer_answer_correctness.single_turn_ascore(sample)\n",
    "            result_item[\"answer_correctness\"] = correctness_score\n",
    "            metric_sums[\"answer_correctness\"] += correctness_score\n",
    "\n",
    "        if \"semantic_similarity\" in metrics:\n",
    "            semsim_score = await scorer_semantic_similarity.single_turn_ascore(sample)\n",
    "            result_item[\"semantic_similarity\"] = semsim_score\n",
    "            metric_sums[\"semantic_similarity\"] += semsim_score\n",
    "\n",
    "        if \"bleu\" in metrics:\n",
    "            bleu_score_val = await bleu_scorer.single_turn_ascore(sample)\n",
    "            result_item[\"bleu\"] = bleu_score_val\n",
    "            metric_sums[\"bleu\"] += bleu_score_val\n",
    "\n",
    "        if \"rouge\" in metrics:\n",
    "            for rouge_type, modes in rouge_combinations.items():\n",
    "                for mode in modes:\n",
    "                    scorer = RougeScore(rouge_type=rouge_type, mode=mode)\n",
    "                    rouge_val = await scorer.single_turn_ascore(sample)\n",
    "                    column_name = f\"{rouge_type}_{mode}\"\n",
    "                    result_item[column_name] = rouge_val\n",
    "                    metric_sums[column_name] += rouge_val\n",
    "\n",
    "        item_results.append(result_item)\n",
    "        count_items += 1\n",
    "\n",
    "        individual_filename = os.path.join(results_dir, f\"answer_{q_number}.json\")\n",
    "        with open(individual_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result_item, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[{idx}/{total_items}] Questão {q_number} processada e salva em {individual_filename}.\", flush=True)\n",
    "\n",
    "    final_scores = average_metric_sums(metric_sums, count_items)\n",
    "    output_data = {\n",
    "        \"item_results\": item_results,\n",
    "        \"overall_scores\": final_scores\n",
    "    }\n",
    "\n",
    "    all_results_filename = os.path.join(results_dir, output_json)\n",
    "    with open(all_results_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Todos os resultados foram salvos em {all_results_filename}.\")\n",
    "    return output_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5aecad-4a92-4cfd-8edc-21459ab13c23",
   "metadata": {},
   "source": [
    "Execução da Avaliação de Métricas\n",
    "Nesta seção, definimos os parâmetros, carregamos os dados (arquivo JSON) e executamos a função evaluate_metrics para processar o dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88e507b0-9c05-440d-b85b-2cf41b2e021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200] Questão 1 processada e salva em results_notebook\\answer_1.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defina os parâmetros (ajuste os caminhos e modelos conforme necessário)\n",
    "filename = \"files/data.json\"  # Atualize para o caminho correto do seu arquivo que contém as respostas\n",
    "reference_column = \"answer\"\n",
    "candidate_column = \"chatgpt(4o/4mini)+search\"\n",
    "question_column = \"question_text\"\n",
    "metrics = [\"answer_relevancy\", \"answer_correctness\", \"semantic_similarity\", \"bleu\", \"rouge\"]\n",
    "llm_model = \"gpt-4o-mini\" # Modelo de LLM a ser utilizado\n",
    "embedding_model = \"text-embedding-3-small\" # ou 'text-embedding-3-large' / 'text-embedding-ada-002'\n",
    "output_json = \"notebook_results.json\"\n",
    "\n",
    "# Carregar os dados do arquivo JSON\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Executar a avaliação de métricas\n",
    "results = asyncio.run(evaluate_metrics(\n",
    "    data=data,\n",
    "    question_col=question_column,\n",
    "    reference_col=reference_column,\n",
    "    candidate_col=candidate_column,\n",
    "    metrics=metrics,\n",
    "    llm_model=llm_model,\n",
    "    embedding_model=embedding_model,\n",
    "    output_json=output_json\n",
    "))\n",
    "\n",
    "print(\"\\n=== MÉTRICAS GLOBAIS ===\")\n",
    "print(json.dumps(results[\"overall_scores\"], indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003a01-86ac-4ad5-b81f-7df37d12f3c4",
   "metadata": {},
   "source": [
    "Consolidação dos Resultados (Usar apenas se necessário, o código anterior já salva os resultados.)\n",
    "Nesta seção, consolidamos os arquivos individuais answer_*.json salvos na pasta results, calculando a média das métricas e gerando o arquivo all_results.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5862b537-416a-4f27-a798-0647c5594884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "results_dir = \"results\"  # Ajuste se necessário\n",
    "file_pattern = os.path.join(results_dir, \"answer_*.json\")\n",
    "file_list = sorted(\n",
    "    glob.glob(file_pattern),\n",
    "    key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('_')[1])\n",
    ")\n",
    "\n",
    "if not file_list:\n",
    "    print(\"Nenhum arquivo answer_*.json encontrado em\", results_dir)\n",
    "else:\n",
    "    item_results = []\n",
    "    non_metric_keys = {\"question_number\", \"question_text\", \"reference\", \"candidate\"}\n",
    "    metric_sums = {}\n",
    "    count_items = 0\n",
    "\n",
    "    for file in file_list:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data_item = json.load(f)\n",
    "            item_results.append(data_item)\n",
    "            count_items += 1\n",
    "            for key, value in data_item.items():\n",
    "                if key not in non_metric_keys and isinstance(value, (int, float)):\n",
    "                    metric_sums[key] = metric_sums.get(key, 0.0) + value\n",
    "\n",
    "    overall_scores = average_metric_sums(metric_sums, count_items)\n",
    "    output_data = {\n",
    "        \"item_results\": item_results,\n",
    "        \"overall_scores\": overall_scores\n",
    "    }\n",
    "\n",
    "    output_filename = os.path.join(results_dir, \"all_results.json\")\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Todos os resultados foram salvos em {output_filename}.\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
